{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf01516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input,decode_predictions\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "image_size = (224, 224)\n",
    "\n",
    "model_mobileNet =  tf.keras.applications.MobileNet(\n",
    "    input_shape=image_size + (3,),\n",
    "    alpha=1.0,\n",
    "    depth_multiplier=1,\n",
    "    dropout=0.001,\n",
    "    include_top=True,\n",
    "    weights=None,\n",
    "    input_tensor=None,\n",
    "    pooling=None,\n",
    "    classes=1,\n",
    "    classifier_activation=\"sigmoid\",\n",
    ")\n",
    "model_mobileNet_with_GAN =  tf.keras.applications.MobileNet(\n",
    "    input_shape=image_size + (3,),\n",
    "    alpha=1.0,\n",
    "    depth_multiplier=1,\n",
    "    dropout=0.001,\n",
    "    include_top=True,\n",
    "    weights=None,\n",
    "    input_tensor=None,\n",
    "    pooling=None,\n",
    "    classes=1,\n",
    "    classifier_activation=\"sigmoid\",\n",
    ")\n",
    "#test_file_path = {'11.jpg','1.jpg'}#массив путей до тестовых изображений\n",
    "model_list = {'vgg16' : model_vgg16,\n",
    "              'resnet50': model_resnet50,\n",
    "              'mobileNet':model_mobileNet}\n",
    "def get_class(predictions):\n",
    "    return decode_predictions(predictions,top = 1)[0][0][1]\n",
    "def get_prob(predictions):\n",
    "    return decode_predictions(predictions,top = 1)[0][0][2]\n",
    "\n",
    "def img_to_arr(img):\n",
    "    img = keras.preprocessing.image.load_img(\n",
    "        file_path, target_size=(224,224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    return img_array\n",
    "def predict_(model, filepath):\n",
    "    img = keras.preprocessing.image.load_img(file_path, target_size=(224,224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    preds = model_list.get(model).predict(arr)\n",
    "    print('Модель:{} Класс: {}, вероятность: {}\\n'.format(model,get_class(preds),get_prob(preds)))\n",
    "    return preds \n",
    "#тестирование сетей\n",
    "# for model in model_list.keys():\n",
    "#     for img in test_file_path:\n",
    "#         predict_(model,img)\n",
    "\n",
    "#добавить датасет +предобработать\n",
    "\n",
    "#добавить список генераторов шумов\n",
    "#добавить генеративно состязательную сеть\n",
    "#добавить стандартные методы аугментации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc8b45cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def adv_acc(model, clear_image_path , dirty_image_path):\n",
    "    TP,TN,FT,FN = 0,0,0,0\n",
    "    predict_clear_cat = get_class(predict_(model,list_of_clear_cat))\n",
    "    predict_dirty_cat = get_class(predict_(model,list_of_dirty_cat))\n",
    "    \n",
    "    predict_clear_dog = get_class(predict_(model,list_of_clear_dog))\n",
    "    predict_dirty_dog = get_class(predict_(model,list_of_dirty_dog))\n",
    "    \n",
    "    if predict_cleat_cat =='cat' and predict_dirty_cat =='cat' or predict_clear_dog =='dog' and predict_dirty_dog =='dog':\n",
    "        TP+=1\n",
    "    if predict_cleat_cat !='cat' and predict_dirty_cat !='cat' or predict_clear_dog !='dog' and predict_dirty_dog !='dog':\n",
    "        TN+=1\n",
    "    if predict_cleat_cat =='cat' and predict_dirty_cat !='cat' or predict_clear_dog =='dog' and predict_dirty_dog !='dog':\n",
    "        FN+=1\n",
    "    if predict_cleat_cat !='cat' and predict_dirty_cat =='cat' or predict_clear_dog !='dog' and predict_dirty_dog =='dog':\n",
    "        FP+=1    \n",
    "    atr_acc = (TP + TN)/(TP + TN + FN + FP)\n",
    "    return atr_acc\n",
    "\n",
    "\n",
    "def diff_pred(model, clear_image_path , dirty_image_path):\n",
    "    summ = 0\n",
    "    for img,img_d in list_of_clear_data_cat,list_of_dirty_data_cat:\n",
    "        predict_clear_cat = get_prob(predict_(model,list_of_clear_cat))\n",
    "        predict_dirty_cat = get_prob(predict_(model,list_of_dirty_cat))\n",
    "        summ += predict_clear_cat - predict_dirty_cat\n",
    "    for img,img_d in list_of_clear_data_dog,list_of_dirty_data_dog:\n",
    "        predict_clear_dog = get_prob(predict_(model,list_of_clear_dog))\n",
    "        predict_dirty_dog = get_prob(predict_(model,list_of_dirty_dog))\n",
    "        summ += predict_clear_dog - predict_dirty_dog\n",
    "    return summ/(len(list_of_clear_data_cat) + len(list_of_clear_data_dog))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb45ad8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 0 images\n",
      "Found 23422 files belonging to 2 classes.\n",
      "Using 18738 files for training.\n",
      "Found 23422 files belonging to 2 classes.\n",
      "Using 4684 files for validation.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "num_skipped = 0\n",
    "for folder_name in (\"Cat\", \"Dog\"):\n",
    "    folder_path = os.path.join(\"PetImages\", folder_name)\n",
    "    for fname in os.listdir(folder_path):\n",
    "        fpath = os.path.join(folder_path, fname)\n",
    "        try:\n",
    "            fobj = open(fpath, \"rb\")\n",
    "            is_jfif = tf.compat.as_bytes(\"JFIF\") in fobj.peek(10)\n",
    "        finally:\n",
    "            fobj.close()\n",
    "\n",
    "        if not is_jfif:\n",
    "            num_skipped += 1\n",
    "            # Delete corrupted image\n",
    "            os.remove(fpath)\n",
    "\n",
    "print(\"Deleted %d images\" % num_skipped)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"PetImages\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=1337,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"PetImages\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=1337,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "         layers.RandomFlip(\"horizontal\"),\n",
    "         layers.RandomRotation(0.1),\n",
    "    ]\n",
    ")\n",
    "inputs = keras.Input(shape=image_size + (3,))\n",
    "x = data_augmentation(inputs)\n",
    "x = layers.Rescaling(1./255)(x)\n",
    "\n",
    "train_ds = train_ds.prefetch(buffer_size=32)\n",
    "val_ds = val_ds.prefetch(buffer_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dea3c15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_vgg16 = keras.models.load_model('vgg16_save_at_11.h5')\n",
    "# model_vgg16.compile(\n",
    "#     optimizer=keras.optimizers.Adam(1e-3),\n",
    "#     loss=\"binary_crossentropy\",\n",
    "#     metrics=[\"accuracy\"],\n",
    "# )\n",
    "# callbacks_for_vgg16 = [\n",
    "#     keras.callbacks.ModelCheckpoint(\"2_vgg16_save_at_{epoch}.h5\"),\n",
    "# ]\n",
    "# model_vgg16.fit(\n",
    "#     train_ds, epochs=39, callbacks = callbacks_for_vgg16, validation_data=val_ds,\n",
    "# )\n",
    "# model_resnet50.compile(\n",
    "#     optimizer=keras.optimizers.Adam(1e-3),\n",
    "#     loss=\"binary_crossentropy\",\n",
    "#     metrics=[\"accuracy\"],\n",
    "# )\n",
    "\n",
    "\n",
    "# callbacks_for_resnet50 = [\n",
    "#     keras.callbacks.ModelCheckpoint(\"resnet50_save_at_{epoch}.h5\"),\n",
    "# ]\n",
    "\n",
    "# model_resnet50.fit(\n",
    "#     train_ds, epochs=50, callbacks = callbacks_for_resnet50, validation_data=val_ds,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fcd93aa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "586/586 [==============================] - 1405s 2s/step - loss: 0.6476 - accuracy: 0.6298 - val_loss: 0.7014 - val_accuracy: 0.5009\n",
      "Epoch 2/50\n",
      "586/586 [==============================] - 1367s 2s/step - loss: 0.5319 - accuracy: 0.7309 - val_loss: 0.6580 - val_accuracy: 0.7306\n",
      "Epoch 3/50\n",
      "586/586 [==============================] - 1398s 2s/step - loss: 0.4509 - accuracy: 0.7883 - val_loss: 0.6449 - val_accuracy: 0.7237\n",
      "Epoch 4/50\n",
      "586/586 [==============================] - 1360s 2s/step - loss: 0.3806 - accuracy: 0.8265 - val_loss: 0.8186 - val_accuracy: 0.7385\n",
      "Epoch 5/50\n",
      "586/586 [==============================] - 1347s 2s/step - loss: 0.3174 - accuracy: 0.8610 - val_loss: 0.5366 - val_accuracy: 0.7908\n",
      "Epoch 6/50\n",
      "586/586 [==============================] - 1497s 3s/step - loss: 0.2654 - accuracy: 0.8872 - val_loss: 0.4259 - val_accuracy: 0.8474\n",
      "Epoch 7/50\n",
      "586/586 [==============================] - 1385s 2s/step - loss: 0.2296 - accuracy: 0.9043 - val_loss: 0.4888 - val_accuracy: 0.8260\n",
      "Epoch 8/50\n",
      "586/586 [==============================] - 1387s 2s/step - loss: 0.1986 - accuracy: 0.9190 - val_loss: 0.2898 - val_accuracy: 0.8841\n",
      "Epoch 9/50\n",
      "586/586 [==============================] - 1386s 2s/step - loss: 0.1704 - accuracy: 0.9327 - val_loss: 1.3794 - val_accuracy: 0.6507\n",
      "Epoch 10/50\n",
      "586/586 [==============================] - 1386s 2s/step - loss: 0.1470 - accuracy: 0.9401 - val_loss: 0.5786 - val_accuracy: 0.8482\n",
      "Epoch 11/50\n",
      "586/586 [==============================] - 1389s 2s/step - loss: 0.1290 - accuracy: 0.9480 - val_loss: 1.3025 - val_accuracy: 0.7267\n",
      "Epoch 12/50\n",
      "586/586 [==============================] - 1389s 2s/step - loss: 0.1206 - accuracy: 0.9518 - val_loss: 0.2966 - val_accuracy: 0.8984\n",
      "Epoch 13/50\n",
      "586/586 [==============================] - 1389s 2s/step - loss: 0.0960 - accuracy: 0.9618 - val_loss: 0.2566 - val_accuracy: 0.9012\n",
      "Epoch 14/50\n",
      "586/586 [==============================] - 1394s 2s/step - loss: 0.0894 - accuracy: 0.9669 - val_loss: 0.2905 - val_accuracy: 0.9063\n",
      "Epoch 15/50\n",
      "586/586 [==============================] - 1387s 2s/step - loss: 0.0761 - accuracy: 0.9710 - val_loss: 0.5084 - val_accuracy: 0.8807\n",
      "Epoch 16/50\n",
      "586/586 [==============================] - 1380s 2s/step - loss: 0.0677 - accuracy: 0.9729 - val_loss: 0.3244 - val_accuracy: 0.9103\n",
      "Epoch 17/50\n",
      "586/586 [==============================] - 1394s 2s/step - loss: 0.0683 - accuracy: 0.9748 - val_loss: 0.8523 - val_accuracy: 0.8303\n",
      "Epoch 18/50\n",
      "586/586 [==============================] - 1402s 2s/step - loss: 0.0560 - accuracy: 0.9790 - val_loss: 0.3334 - val_accuracy: 0.9133\n",
      "Epoch 19/50\n",
      "586/586 [==============================] - 1395s 2s/step - loss: 0.0537 - accuracy: 0.9807 - val_loss: 0.3371 - val_accuracy: 0.9084\n",
      "Epoch 20/50\n",
      "586/586 [==============================] - 1391s 2s/step - loss: 0.0442 - accuracy: 0.9836 - val_loss: 0.3957 - val_accuracy: 0.9007\n",
      "Epoch 21/50\n",
      "586/586 [==============================] - 1394s 2s/step - loss: 0.0492 - accuracy: 0.9815 - val_loss: 0.4359 - val_accuracy: 0.9024\n",
      "Epoch 22/50\n",
      "586/586 [==============================] - 1398s 2s/step - loss: 0.0354 - accuracy: 0.9860 - val_loss: 0.3692 - val_accuracy: 0.9148\n",
      "Epoch 23/50\n",
      "586/586 [==============================] - 1395s 2s/step - loss: 0.0381 - accuracy: 0.9857 - val_loss: 0.5182 - val_accuracy: 0.8907\n",
      "Epoch 24/50\n",
      "586/586 [==============================] - 1400s 2s/step - loss: 0.0381 - accuracy: 0.9857 - val_loss: 0.3188 - val_accuracy: 0.9270\n",
      "Epoch 25/50\n",
      "586/586 [==============================] - 1397s 2s/step - loss: 0.0312 - accuracy: 0.9884 - val_loss: 0.3092 - val_accuracy: 0.9229\n",
      "Epoch 26/50\n",
      "586/586 [==============================] - 1400s 2s/step - loss: 0.0333 - accuracy: 0.9879 - val_loss: 0.4626 - val_accuracy: 0.9067\n",
      "Epoch 27/50\n",
      "586/586 [==============================] - 1403s 2s/step - loss: 0.0376 - accuracy: 0.9862 - val_loss: 0.2583 - val_accuracy: 0.9133\n",
      "Epoch 28/50\n",
      "586/586 [==============================] - 1402s 2s/step - loss: 0.0287 - accuracy: 0.9901 - val_loss: 0.3066 - val_accuracy: 0.9263\n",
      "Epoch 29/50\n",
      "586/586 [==============================] - 1403s 2s/step - loss: 0.0274 - accuracy: 0.9908 - val_loss: 0.2900 - val_accuracy: 0.9315\n",
      "Epoch 30/50\n",
      "586/586 [==============================] - 1404s 2s/step - loss: 0.0282 - accuracy: 0.9905 - val_loss: 0.3533 - val_accuracy: 0.9063\n",
      "Epoch 31/50\n",
      "586/586 [==============================] - 1440s 2s/step - loss: 0.0266 - accuracy: 0.9899 - val_loss: 0.4304 - val_accuracy: 0.8999\n",
      "Epoch 32/50\n",
      "586/586 [==============================] - 1330s 2s/step - loss: 0.0261 - accuracy: 0.9914 - val_loss: 0.2808 - val_accuracy: 0.9268\n",
      "Epoch 33/50\n",
      "586/586 [==============================] - 1312s 2s/step - loss: 0.0226 - accuracy: 0.9920 - val_loss: 0.2996 - val_accuracy: 0.9214\n",
      "Epoch 34/50\n",
      "586/586 [==============================] - 1330s 2s/step - loss: 0.0237 - accuracy: 0.9919 - val_loss: 0.2563 - val_accuracy: 0.9302\n",
      "Epoch 35/50\n",
      "586/586 [==============================] - 1332s 2s/step - loss: 0.0213 - accuracy: 0.9922 - val_loss: 0.3600 - val_accuracy: 0.9231\n",
      "Epoch 36/50\n",
      "586/586 [==============================] - 1324s 2s/step - loss: 0.0194 - accuracy: 0.9936 - val_loss: 0.8125 - val_accuracy: 0.8755\n",
      "Epoch 37/50\n",
      "586/586 [==============================] - 1333s 2s/step - loss: 0.0264 - accuracy: 0.9908 - val_loss: 0.3268 - val_accuracy: 0.9272\n",
      "Epoch 38/50\n",
      "586/586 [==============================] - 1373s 2s/step - loss: 0.0162 - accuracy: 0.9942 - val_loss: 0.2862 - val_accuracy: 0.9302\n",
      "Epoch 39/50\n",
      "586/586 [==============================] - 1374s 2s/step - loss: 0.0159 - accuracy: 0.9943 - val_loss: 0.2835 - val_accuracy: 0.9319\n",
      "Epoch 40/50\n",
      "586/586 [==============================] - 1467s 3s/step - loss: 0.0225 - accuracy: 0.9925 - val_loss: 0.2903 - val_accuracy: 0.9261\n",
      "Epoch 41/50\n",
      "586/586 [==============================] - 1497s 3s/step - loss: 0.0165 - accuracy: 0.9943 - val_loss: 0.5574 - val_accuracy: 0.9029\n",
      "Epoch 42/50\n",
      "586/586 [==============================] - 1553s 3s/step - loss: 0.0166 - accuracy: 0.9943 - val_loss: 0.2469 - val_accuracy: 0.9381\n",
      "Epoch 43/50\n",
      "586/586 [==============================] - 1575s 3s/step - loss: 0.0175 - accuracy: 0.9932 - val_loss: 0.2720 - val_accuracy: 0.9345\n",
      "Epoch 44/50\n",
      "586/586 [==============================] - 2041s 3s/step - loss: 0.0187 - accuracy: 0.9932 - val_loss: 0.2803 - val_accuracy: 0.9253\n",
      "Epoch 45/50\n",
      "586/586 [==============================] - 1391s 2s/step - loss: 0.0150 - accuracy: 0.9951 - val_loss: 0.2703 - val_accuracy: 0.9327\n",
      "Epoch 46/50\n",
      "586/586 [==============================] - 1381s 2s/step - loss: 0.0129 - accuracy: 0.9957 - val_loss: 0.3316 - val_accuracy: 0.9268\n",
      "Epoch 47/50\n",
      "586/586 [==============================] - 1370s 2s/step - loss: 0.0138 - accuracy: 0.9957 - val_loss: 0.3121 - val_accuracy: 0.9259\n",
      "Epoch 48/50\n",
      "586/586 [==============================] - 1367s 2s/step - loss: 0.0134 - accuracy: 0.9951 - val_loss: 0.3674 - val_accuracy: 0.9236\n",
      "Epoch 49/50\n",
      "586/586 [==============================] - 1371s 2s/step - loss: 0.0152 - accuracy: 0.9949 - val_loss: 0.2916 - val_accuracy: 0.9332\n",
      "Epoch 50/50\n",
      "586/586 [==============================] - 1371s 2s/step - loss: 0.0148 - accuracy: 0.9946 - val_loss: 0.2679 - val_accuracy: 0.9389\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x205d7640dc0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# модель MobileNet обучена на нормальных данных\n",
    "model_mobileNet.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-3),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "callbacks_for_mobilenet = [\n",
    "    keras.callbacks.ModelCheckpoint(\"mobilenet_save_at_{epoch}.h5\"),\n",
    "]\n",
    "\n",
    "model_mobileNet.fit(\n",
    "    train_ds, epochs=50, callbacks = callbacks_for_mobilenet, validation_data=val_ds,\n",
    ")\n",
    "#дальше зашумить картинки - предикт всех картинок(и нормальных и зашумленных и пропуск через функции метрик)\n",
    "#дальше обучить на датасете с грязными данными и повторить прогон метрик\n",
    "#дальше применить встроенные функции аугментации и повторить прогон метрик\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc05b45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_clear_data_cat = os.listdir('PetImages/Cat')\n",
    "# list_of_clear_data_dog = os.listdir('PetImages/Dog')\n",
    "# list_of_dirty_data_cat = os.listdir('PetImages_dirty/Cat')\n",
    "# list_of_dirty_data_dog = os.listdir('PetImages_dirty/Dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c37cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "# #Тестировать только после обучения модели.\n",
    "# def create_adversarial_pattern(input_image, #изображение которое предсказываем\n",
    "#                                input_label, #ожидаемый класс\n",
    "#                                model):#модель НС\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         tape.watch(input_image)\n",
    "#         prediction = predict_(model,input_label)\n",
    "#         loss = loss_object(predict_(, prediction)\n",
    "\n",
    "#     # Get the gradients of the loss w.r.t to the input image.\n",
    "#     gradient = tape.gradient(loss, input_image)\n",
    "#     # Get the sign of the gradients to create the perturbation\n",
    "#     signed_grad = tf.sign(gradient)\n",
    "#     return signed_grad\n",
    "# #массив предсказаний класса - input_label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3482aeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for img in list_of_clear_data_cat:\n",
    "#     perturbations = create_adversarial_pattern(img, get_class())\n",
    "#     new_img = open(\"PetImages_dirty/Cat\" + img,\"w\")\n",
    "#     new_img.write('PetImages/Cat'+ img + perturbations)\n",
    "#     new_img.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "13f0ad73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 33748 files belonging to 2 classes.\n",
      "Using 26999 files for training.\n",
      "Found 33748 files belonging to 2 classes.\n",
      "Using 6749 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_ds_with_gan = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"PG\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=1337,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "val_ds_with_gan = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"PG\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=1337,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d67d2cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "844/844 [==============================] - 4127s 5s/step - loss: 0.6006 - accuracy: 0.6680 - val_loss: 0.5420 - val_accuracy: 0.7274\n",
      "Epoch 2/50\n",
      "844/844 [==============================] - 2030s 2s/step - loss: 0.4200 - accuracy: 0.8012 - val_loss: 0.4164 - val_accuracy: 0.8124\n",
      "Epoch 3/50\n",
      "844/844 [==============================] - 2040s 2s/step - loss: 0.3018 - accuracy: 0.8656 - val_loss: 0.2661 - val_accuracy: 0.8840\n",
      "Epoch 4/50\n",
      "844/844 [==============================] - 2035s 2s/step - loss: 0.2242 - accuracy: 0.9050 - val_loss: 0.5886 - val_accuracy: 0.8068\n",
      "Epoch 5/50\n",
      "844/844 [==============================] - 2033s 2s/step - loss: 0.1832 - accuracy: 0.9243 - val_loss: 0.3085 - val_accuracy: 0.8791\n",
      "Epoch 6/50\n",
      "844/844 [==============================] - 2035s 2s/step - loss: 0.1500 - accuracy: 0.9389 - val_loss: 0.2975 - val_accuracy: 0.8939\n",
      "Epoch 7/50\n",
      "844/844 [==============================] - 2001s 2s/step - loss: 0.1263 - accuracy: 0.9486 - val_loss: 0.3543 - val_accuracy: 0.8791\n",
      "Epoch 8/50\n",
      "844/844 [==============================] - 2003s 2s/step - loss: 0.1094 - accuracy: 0.9563 - val_loss: 0.2578 - val_accuracy: 0.9074\n",
      "Epoch 9/50\n",
      "844/844 [==============================] - 2012s 2s/step - loss: 0.0920 - accuracy: 0.9638 - val_loss: 0.3131 - val_accuracy: 0.9040\n",
      "Epoch 10/50\n",
      "844/844 [==============================] - 1963s 2s/step - loss: 0.0838 - accuracy: 0.9665 - val_loss: 0.1939 - val_accuracy: 0.9274\n",
      "Epoch 11/50\n",
      "844/844 [==============================] - 1936s 2s/step - loss: 0.0742 - accuracy: 0.9717 - val_loss: 0.1719 - val_accuracy: 0.9378\n",
      "Epoch 12/50\n",
      "844/844 [==============================] - 1949s 2s/step - loss: 0.0588 - accuracy: 0.9777 - val_loss: 0.1636 - val_accuracy: 0.9398\n",
      "Epoch 13/50\n",
      "844/844 [==============================] - 1943s 2s/step - loss: 0.0510 - accuracy: 0.9812 - val_loss: 0.2289 - val_accuracy: 0.9286\n",
      "Epoch 14/50\n",
      "844/844 [==============================] - 1966s 2s/step - loss: 0.0516 - accuracy: 0.9808 - val_loss: 0.2037 - val_accuracy: 0.9350\n",
      "Epoch 15/50\n",
      "844/844 [==============================] - 1951s 2s/step - loss: 0.0425 - accuracy: 0.9846 - val_loss: 0.2141 - val_accuracy: 0.9363\n",
      "Epoch 16/50\n",
      "844/844 [==============================] - 1966s 2s/step - loss: 0.0431 - accuracy: 0.9836 - val_loss: 0.1846 - val_accuracy: 0.9416\n",
      "Epoch 17/50\n",
      "844/844 [==============================] - 1959s 2s/step - loss: 0.0313 - accuracy: 0.9888 - val_loss: 0.2037 - val_accuracy: 0.9441\n",
      "Epoch 18/50\n",
      "844/844 [==============================] - 1955s 2s/step - loss: 0.0327 - accuracy: 0.9879 - val_loss: 0.1777 - val_accuracy: 0.9475\n",
      "Epoch 19/50\n",
      "844/844 [==============================] - 1953s 2s/step - loss: 0.0287 - accuracy: 0.9895 - val_loss: 0.2315 - val_accuracy: 0.9308\n",
      "Epoch 20/50\n",
      "844/844 [==============================] - 1935s 2s/step - loss: 0.0254 - accuracy: 0.9911 - val_loss: 0.2533 - val_accuracy: 0.9338\n",
      "Epoch 21/50\n",
      "844/844 [==============================] - 1895s 2s/step - loss: 0.0280 - accuracy: 0.9896 - val_loss: 0.2172 - val_accuracy: 0.9372\n",
      "Epoch 22/50\n",
      "844/844 [==============================] - 1881s 2s/step - loss: 0.0285 - accuracy: 0.9896 - val_loss: 0.2134 - val_accuracy: 0.9433\n",
      "Epoch 23/50\n",
      "844/844 [==============================] - 1884s 2s/step - loss: 0.0218 - accuracy: 0.9924 - val_loss: 0.1849 - val_accuracy: 0.9493\n",
      "Epoch 24/50\n",
      "844/844 [==============================] - 1879s 2s/step - loss: 0.0221 - accuracy: 0.9918 - val_loss: 0.2713 - val_accuracy: 0.9421\n",
      "Epoch 25/50\n",
      "844/844 [==============================] - 1872s 2s/step - loss: 0.0204 - accuracy: 0.9923 - val_loss: 0.1902 - val_accuracy: 0.9418\n",
      "Epoch 26/50\n",
      "844/844 [==============================] - 2100s 2s/step - loss: 0.0169 - accuracy: 0.9943 - val_loss: 0.2187 - val_accuracy: 0.9446\n",
      "Epoch 27/50\n",
      "844/844 [==============================] - 2084s 2s/step - loss: 0.0159 - accuracy: 0.9948 - val_loss: 0.2222 - val_accuracy: 0.9397\n",
      "Epoch 28/50\n",
      "844/844 [==============================] - 2007s 2s/step - loss: 0.0185 - accuracy: 0.9937 - val_loss: 0.2005 - val_accuracy: 0.9424\n",
      "Epoch 29/50\n",
      "844/844 [==============================] - 2231s 3s/step - loss: 0.0185 - accuracy: 0.9928 - val_loss: 0.2076 - val_accuracy: 0.9484\n",
      "Epoch 30/50\n",
      "844/844 [==============================] - 2318s 3s/step - loss: 0.0112 - accuracy: 0.9961 - val_loss: 0.1889 - val_accuracy: 0.9542\n",
      "Epoch 31/50\n",
      "844/844 [==============================] - 2032s 2s/step - loss: 0.0177 - accuracy: 0.9931 - val_loss: 0.1832 - val_accuracy: 0.9529\n",
      "Epoch 32/50\n",
      "844/844 [==============================] - 1919s 2s/step - loss: 0.0113 - accuracy: 0.9961 - val_loss: 0.2437 - val_accuracy: 0.9452\n",
      "Epoch 33/50\n",
      "844/844 [==============================] - 2125s 3s/step - loss: 0.0188 - accuracy: 0.9934 - val_loss: 0.1621 - val_accuracy: 0.9553\n",
      "Epoch 34/50\n",
      "844/844 [==============================] - 2449s 3s/step - loss: 0.0108 - accuracy: 0.9962 - val_loss: 0.3734 - val_accuracy: 0.9299\n",
      "Epoch 35/50\n",
      "844/844 [==============================] - 2201s 3s/step - loss: 0.0145 - accuracy: 0.9945 - val_loss: 0.2019 - val_accuracy: 0.9567\n",
      "Epoch 36/50\n",
      "844/844 [==============================] - 2744s 3s/step - loss: 0.0130 - accuracy: 0.9959 - val_loss: 0.1715 - val_accuracy: 0.9541\n",
      "Epoch 37/50\n",
      "844/844 [==============================] - 4554s 5s/step - loss: 0.0097 - accuracy: 0.9968 - val_loss: 0.2045 - val_accuracy: 0.9545\n",
      "Epoch 38/50\n",
      "844/844 [==============================] - 3000s 4s/step - loss: 0.0114 - accuracy: 0.9961 - val_loss: 0.2169 - val_accuracy: 0.9518\n",
      "Epoch 39/50\n",
      "844/844 [==============================] - 1977s 2s/step - loss: 0.0128 - accuracy: 0.9958 - val_loss: 0.2350 - val_accuracy: 0.9518\n",
      "Epoch 40/50\n",
      "844/844 [==============================] - 2060s 2s/step - loss: 0.0126 - accuracy: 0.9957 - val_loss: 0.1729 - val_accuracy: 0.9587\n",
      "Epoch 41/50\n",
      "844/844 [==============================] - 2038s 2s/step - loss: 0.0101 - accuracy: 0.9967 - val_loss: 0.2970 - val_accuracy: 0.9425\n",
      "Epoch 42/50\n",
      "844/844 [==============================] - 1974s 2s/step - loss: 0.0094 - accuracy: 0.9971 - val_loss: 0.1989 - val_accuracy: 0.9612\n",
      "Epoch 43/50\n",
      "844/844 [==============================] - 2065s 2s/step - loss: 0.0113 - accuracy: 0.9960 - val_loss: 0.2612 - val_accuracy: 0.9424\n",
      "Epoch 44/50\n",
      "844/844 [==============================] - 2346s 3s/step - loss: 0.0107 - accuracy: 0.9965 - val_loss: 0.2390 - val_accuracy: 0.9495\n",
      "Epoch 45/50\n",
      "844/844 [==============================] - 4323s 5s/step - loss: 0.0108 - accuracy: 0.9961 - val_loss: 0.2122 - val_accuracy: 0.9548\n",
      "Epoch 46/50\n",
      "844/844 [==============================] - 3508s 4s/step - loss: 0.0083 - accuracy: 0.9969 - val_loss: 0.1943 - val_accuracy: 0.9538\n",
      "Epoch 47/50\n",
      "844/844 [==============================] - 2129s 3s/step - loss: 0.0097 - accuracy: 0.9967 - val_loss: 0.1966 - val_accuracy: 0.9539\n",
      "Epoch 48/50\n",
      "844/844 [==============================] - 1986s 2s/step - loss: 0.0086 - accuracy: 0.9974 - val_loss: 0.2069 - val_accuracy: 0.9573\n",
      "Epoch 49/50\n",
      "844/844 [==============================] - 1957s 2s/step - loss: 0.0110 - accuracy: 0.9962 - val_loss: 0.1771 - val_accuracy: 0.9555\n",
      "Epoch 50/50\n",
      "844/844 [==============================] - 2195s 3s/step - loss: 0.0059 - accuracy: 0.9978 - val_loss: 0.2032 - val_accuracy: 0.9587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2071b14fa30>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#модель MobileNet обучена на выборке+сгенерированные примеры\n",
    "#model_mobileNet_with_GAN = keras.models.load_model('mobilenet_save_at_50.h5')\n",
    "#на случай нежелательных результатов - дообучить основную модель ТОЛЬКО на сгенерированных данных\n",
    "#model_mobileNet_with_GAN = keras.models.load_model('mobilenet_with_gan_save_at_1.h5')\n",
    "model_mobileNet_with_GAN.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-3),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "callbacks_for_mobilenet_with_gan = [\n",
    "    keras.callbacks.ModelCheckpoint(\"mobilenet_with_gan_save_at_{epoch}.h5\"),\n",
    "]\n",
    "model_mobileNet_with_GAN.fit(\n",
    "    train_ds_with_gan, epochs=50, callbacks=callbacks_for_mobilenet_with_gan, validation_data=val_ds_with_gan,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4624c101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23422 files belonging to 2 classes.\n",
      "Using 18738 files for training.\n",
      "Found 23422 files belonging to 2 classes.\n",
      "Using 4684 files for validation.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_ds_with_aug = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"PetImages\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=1337,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode = 'binary'\n",
    ")\n",
    "val_ds_with_aug = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"PetImages\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=1337,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode = 'binary'\n",
    ")\n",
    "train_ds_with_aug = train_ds_with_aug.prefetch(buffer_size=32)\n",
    "val_ds_with_aug = val_ds_with_aug.prefetch(buffer_size=32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3fc386be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "         layers.RandomFlip(\"horizontal\"),\n",
    "         layers.RandomRotation(0.1),\n",
    "        \n",
    "    ]\n",
    ")\n",
    "inputs = keras.Input(shape=image_size + (3,))\n",
    "x = data_augmentation(inputs)\n",
    "x = layers.Rescaling(1./255)(x)\n",
    "\n",
    "train_ds_with_aug = train_ds_with_aug.prefetch(buffer_size=32)\n",
    "val_ds_with_aug = val_ds_with_aug.prefetch(buffer_size=32)\n",
    "model_mobileNet_with_aug =  tf.keras.applications.MobileNet(\n",
    "    input_shape=image_size + (3,),\n",
    "    alpha=1.0,\n",
    "    depth_multiplier=1,\n",
    "    dropout=0.001,\n",
    "    include_top=True,\n",
    "    weights=None,\n",
    "    input_tensor=x,\n",
    "    pooling=None,\n",
    "    classes=1,\n",
    "    classifier_activation=\"sigmoid\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e778b06a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "586/586 [==============================] - 1429s 2s/step - loss: 0.6566 - accuracy: 0.6216 - val_loss: 1.0268 - val_accuracy: 0.4957\n",
      "Epoch 2/50\n",
      "586/586 [==============================] - 1448s 2s/step - loss: 0.5703 - accuracy: 0.6983 - val_loss: 0.6509 - val_accuracy: 0.6386\n",
      "Epoch 3/50\n",
      "586/586 [==============================] - 1475s 3s/step - loss: 0.5002 - accuracy: 0.7530 - val_loss: 0.6741 - val_accuracy: 0.6808\n",
      "Epoch 4/50\n",
      "586/586 [==============================] - 1486s 3s/step - loss: 0.4513 - accuracy: 0.7846 - val_loss: 0.4984 - val_accuracy: 0.7487\n",
      "Epoch 5/50\n",
      "586/586 [==============================] - 1484s 3s/step - loss: 0.4004 - accuracy: 0.8176 - val_loss: 0.5281 - val_accuracy: 0.7504\n",
      "Epoch 6/50\n",
      "586/586 [==============================] - 1494s 3s/step - loss: 0.3464 - accuracy: 0.8443 - val_loss: 0.3965 - val_accuracy: 0.8320\n",
      "Epoch 7/50\n",
      "586/586 [==============================] - 1439s 2s/step - loss: 0.2975 - accuracy: 0.8704 - val_loss: 0.4472 - val_accuracy: 0.8254\n",
      "Epoch 8/50\n",
      "586/586 [==============================] - 1483s 3s/step - loss: 0.2731 - accuracy: 0.8824 - val_loss: 0.2543 - val_accuracy: 0.8918\n",
      "Epoch 9/50\n",
      "586/586 [==============================] - 1420s 2s/step - loss: 0.2491 - accuracy: 0.8934 - val_loss: 0.2425 - val_accuracy: 0.8945\n",
      "Epoch 10/50\n",
      "586/586 [==============================] - 1426s 2s/step - loss: 0.2286 - accuracy: 0.9040 - val_loss: 0.2546 - val_accuracy: 0.8958\n",
      "Epoch 11/50\n",
      "586/586 [==============================] - 1417s 2s/step - loss: 0.2123 - accuracy: 0.9122 - val_loss: 0.2054 - val_accuracy: 0.9159\n",
      "Epoch 12/50\n",
      "586/586 [==============================] - 1426s 2s/step - loss: 0.1976 - accuracy: 0.9187 - val_loss: 0.2040 - val_accuracy: 0.9170\n",
      "Epoch 13/50\n",
      "586/586 [==============================] - 1413s 2s/step - loss: 0.1885 - accuracy: 0.9221 - val_loss: 0.4179 - val_accuracy: 0.8029\n",
      "Epoch 14/50\n",
      "586/586 [==============================] - 1413s 2s/step - loss: 0.1690 - accuracy: 0.9304 - val_loss: 0.2057 - val_accuracy: 0.9131\n",
      "Epoch 15/50\n",
      "586/586 [==============================] - 1416s 2s/step - loss: 0.1632 - accuracy: 0.9339 - val_loss: 0.3556 - val_accuracy: 0.8845\n",
      "Epoch 16/50\n",
      "586/586 [==============================] - 1406s 2s/step - loss: 0.1535 - accuracy: 0.9381 - val_loss: 0.2366 - val_accuracy: 0.9067\n",
      "Epoch 17/50\n",
      "586/586 [==============================] - 1412s 2s/step - loss: 0.1439 - accuracy: 0.9422 - val_loss: 0.2182 - val_accuracy: 0.9052\n",
      "Epoch 18/50\n",
      "586/586 [==============================] - 1428s 2s/step - loss: 0.1402 - accuracy: 0.9431 - val_loss: 0.1533 - val_accuracy: 0.9398\n",
      "Epoch 19/50\n",
      "586/586 [==============================] - 1509s 3s/step - loss: 0.1324 - accuracy: 0.9487 - val_loss: 0.2044 - val_accuracy: 0.9099\n",
      "Epoch 20/50\n",
      "586/586 [==============================] - 1504s 3s/step - loss: 0.1276 - accuracy: 0.9502 - val_loss: 0.1454 - val_accuracy: 0.9430\n",
      "Epoch 21/50\n",
      "586/586 [==============================] - 1453s 2s/step - loss: 0.1190 - accuracy: 0.9528 - val_loss: 0.1204 - val_accuracy: 0.9526\n",
      "Epoch 22/50\n",
      "586/586 [==============================] - 1411s 2s/step - loss: 0.1156 - accuracy: 0.9544 - val_loss: 0.1161 - val_accuracy: 0.9532\n",
      "Epoch 23/50\n",
      "586/586 [==============================] - 1473s 3s/step - loss: 0.1110 - accuracy: 0.9568 - val_loss: 0.1146 - val_accuracy: 0.9556\n",
      "Epoch 24/50\n",
      "586/586 [==============================] - 2321s 4s/step - loss: 0.1034 - accuracy: 0.9597 - val_loss: 0.1278 - val_accuracy: 0.9494\n",
      "Epoch 25/50\n",
      "586/586 [==============================] - 3163s 5s/step - loss: 0.0987 - accuracy: 0.9608 - val_loss: 0.2910 - val_accuracy: 0.8926\n",
      "Epoch 26/50\n",
      "586/586 [==============================] - 3021s 5s/step - loss: 0.0980 - accuracy: 0.9614 - val_loss: 0.1069 - val_accuracy: 0.9575\n",
      "Epoch 27/50\n",
      "586/586 [==============================] - 1461s 2s/step - loss: 0.0977 - accuracy: 0.9619 - val_loss: 0.1085 - val_accuracy: 0.9569\n",
      "Epoch 28/50\n",
      "586/586 [==============================] - 1540s 3s/step - loss: 0.0850 - accuracy: 0.9656 - val_loss: 0.1221 - val_accuracy: 0.9530\n",
      "Epoch 29/50\n",
      "586/586 [==============================] - 2431s 4s/step - loss: 0.0850 - accuracy: 0.9669 - val_loss: 0.1071 - val_accuracy: 0.9582\n",
      "Epoch 30/50\n",
      "586/586 [==============================] - 3071s 5s/step - loss: 0.0820 - accuracy: 0.9679 - val_loss: 0.1133 - val_accuracy: 0.9584\n",
      "Epoch 31/50\n",
      "586/586 [==============================] - 1842s 3s/step - loss: 0.0803 - accuracy: 0.9695 - val_loss: 0.1443 - val_accuracy: 0.9477\n",
      "Epoch 32/50\n",
      "586/586 [==============================] - 1830s 3s/step - loss: 0.0745 - accuracy: 0.9706 - val_loss: 0.0916 - val_accuracy: 0.9637\n",
      "Epoch 33/50\n",
      "586/586 [==============================] - 1916s 3s/step - loss: 0.0749 - accuracy: 0.9702 - val_loss: 0.1524 - val_accuracy: 0.9426\n",
      "Epoch 34/50\n",
      "586/586 [==============================] - 1913s 3s/step - loss: 0.0702 - accuracy: 0.9712 - val_loss: 0.1211 - val_accuracy: 0.9547\n",
      "Epoch 35/50\n",
      "586/586 [==============================] - 1727s 3s/step - loss: 0.0685 - accuracy: 0.9734 - val_loss: 0.1125 - val_accuracy: 0.9556\n",
      "Epoch 36/50\n",
      "586/586 [==============================] - 1473s 3s/step - loss: 0.0672 - accuracy: 0.9743 - val_loss: 0.0986 - val_accuracy: 0.9667\n",
      "Epoch 37/50\n",
      "586/586 [==============================] - 1454s 2s/step - loss: 0.0655 - accuracy: 0.9749 - val_loss: 0.0914 - val_accuracy: 0.9650\n",
      "Epoch 38/50\n",
      "586/586 [==============================] - 1444s 2s/step - loss: 0.0583 - accuracy: 0.9768 - val_loss: 0.0928 - val_accuracy: 0.9631\n",
      "Epoch 39/50\n",
      "586/586 [==============================] - 1437s 2s/step - loss: 0.0606 - accuracy: 0.9764 - val_loss: 0.1486 - val_accuracy: 0.9554\n",
      "Epoch 40/50\n",
      "586/586 [==============================] - 1434s 2s/step - loss: 0.0559 - accuracy: 0.9775 - val_loss: 0.0887 - val_accuracy: 0.9667\n",
      "Epoch 41/50\n",
      "586/586 [==============================] - 1452s 2s/step - loss: 0.0527 - accuracy: 0.9799 - val_loss: 0.0943 - val_accuracy: 0.9641\n",
      "Epoch 42/50\n",
      "586/586 [==============================] - 1449s 2s/step - loss: 0.0527 - accuracy: 0.9790 - val_loss: 0.1110 - val_accuracy: 0.9573\n",
      "Epoch 43/50\n",
      "586/586 [==============================] - 1438s 2s/step - loss: 0.0534 - accuracy: 0.9800 - val_loss: 0.1160 - val_accuracy: 0.9646\n",
      "Epoch 44/50\n",
      "586/586 [==============================] - 1436s 2s/step - loss: 0.0504 - accuracy: 0.9812 - val_loss: 0.0932 - val_accuracy: 0.9661\n",
      "Epoch 45/50\n",
      "586/586 [==============================] - 1433s 2s/step - loss: 0.0505 - accuracy: 0.9815 - val_loss: 0.1092 - val_accuracy: 0.9596\n",
      "Epoch 46/50\n",
      "586/586 [==============================] - 1442s 2s/step - loss: 0.0462 - accuracy: 0.9825 - val_loss: 0.0829 - val_accuracy: 0.9708\n",
      "Epoch 47/50\n",
      "586/586 [==============================] - 1438s 2s/step - loss: 0.0459 - accuracy: 0.9830 - val_loss: 0.1111 - val_accuracy: 0.9592\n",
      "Epoch 48/50\n",
      "586/586 [==============================] - 1444s 2s/step - loss: 0.0464 - accuracy: 0.9826 - val_loss: 0.1191 - val_accuracy: 0.9633\n",
      "Epoch 49/50\n",
      "586/586 [==============================] - 1435s 2s/step - loss: 0.0480 - accuracy: 0.9817 - val_loss: 0.0875 - val_accuracy: 0.9680\n",
      "Epoch 50/50\n",
      "586/586 [==============================] - 1435s 2s/step - loss: 0.0429 - accuracy: 0.9830 - val_loss: 0.0865 - val_accuracy: 0.9690\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2071a32f5b0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mobileNet_with_aug.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-3),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "callbacks_for_mobilenet_with_aug = [\n",
    "    keras.callbacks.ModelCheckpoint(\"mobilenet_with_aug_save_at_{epoch}.h5\"),\n",
    "]\n",
    "model_mobileNet_with_aug.fit(\n",
    "    train_ds_with_aug, epochs=50, callbacks=callbacks_for_mobilenet_with_aug, validation_data=val_ds_with_aug,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd519d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe95a44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
